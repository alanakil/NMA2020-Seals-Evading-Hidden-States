{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code imports the data from the Steinmetz et al. (2019) dataset. First, we extract the data from it's stored form in .tar archives to python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def mylistdir(directory):\n",
    "    \"\"\"A specialized version of os.listdir() that ignores files that\n",
    "    start with a leading period.\"\"\"\n",
    "    filelist = os.listdir(directory)\n",
    "    return [x for x in filelist\n",
    "            if not (x.startswith('.'))]\n",
    "\n",
    "# filepath here should be wherever you have allData (8GB folder of downloaded data) saved\n",
    "folder = 'your/filepath/here'\n",
    "n_sessions = len(os.listdir(folder))\n",
    "\n",
    "for f in mylistdir(folder):\n",
    "    # creating a new folder to save this session's extracted folder\n",
    "    new_folder = os.path.join('your/filepath/here', f'{f}_extracted')\n",
    "    new_path = f'your/filepath/here/{f}'\n",
    "    if new_path.endswith('.tar'):\n",
    "        if os.path.isfile(new_path)==True:\n",
    "            # print('good 2 go')\n",
    "            tar = tarfile.open(new_path, \"r:\")\n",
    "            ## put your filepath that you want to extract to here!! \n",
    "            tar.extractall(f'{new_folder}')\n",
    "            tar.close()          \n",
    "    else:\n",
    "        # print(f'{f} is a directory, can not unzip')\n",
    "        continue  \n",
    "        \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we take the extracted data and load it from .npy files into python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## thank you to https://github.com/Debu922/NMA_Mapping_Brain_Networks_2020/blob/master/notebooks/loadData.ipynb \n",
    "## (/debhh/ on neurostars) who shared a similar version of this code - I adapted it for the data & format \n",
    "## we are using for our project\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "session_paths = glob.glob('your/filepath/here/*')\n",
    "n_sessions=40\n",
    "\n",
    "## The properties of a given object share the same number of n data elements \n",
    "## (specifically, the same number of rows). So spikes.times and spikes.clusters \n",
    "## give two properties of the spikes object, and each have one entry for each spike\n",
    "\n",
    "## object one : neural data\n",
    "\n",
    "# n = spikes\n",
    "spikes_clusters = {} # which cluster # does the spike belong to\n",
    "spikes_times = {}\n",
    "\n",
    "# n = channels\n",
    "channel_brainLocations = {x:[] for x in range(n_sessions)} \n",
    "\n",
    "# n = clusters\n",
    "clusters_phy_annotation = {} # 0 noise, 1 signal coming from multiple neurons, 3 unsorted (only use 2's)\n",
    "clusters_peakChannel = {} # the channel number of the location of the peak of the cluster's waveform\n",
    "\n",
    "## object two : visual discrimnation task\n",
    "# n = trials\n",
    "trial_intervals = {}\n",
    "goCue_times = {}\n",
    "feedback_types = {} # CORRECT // INCORRECT\n",
    "feedback_times = {}\n",
    "response_choices = {} # DIRECTION\n",
    "response_times = {}\n",
    "stimContrast_left = {}\n",
    "stimContrast_right = {}\n",
    "stim_times = {}\n",
    "\n",
    "## object three : behavioral data\n",
    "## n = trials\n",
    "eye_area = {}\n",
    "eye_times = {}\n",
    "face_energy = {}\n",
    "face_times = {}\n",
    "\n",
    "idx = 1\n",
    "\n",
    "for session in session_paths:\n",
    "    \n",
    "    # spikes\n",
    "    spikes_clusters[idx] = np.load(session + '/spikes.clusters.npy', allow_pickle = True)\n",
    "    spikes_times[idx] = np.load(session + '/spikes.times.npy', allow_pickle = True)\n",
    "    \n",
    "    # clusters\n",
    "    clusters_phy_annotation[idx] = np.load(session + '/clusters._phy_annotation.npy', allow_pickle = True)\n",
    "    clusters_peakChannel[idx] = np.load(session + '/clusters.peakChannel.npy', allow_pickle = True)\n",
    "    \n",
    "    # channels\n",
    "    with open(session + '/channels.brainLocation.tsv') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "        for row in reader:\n",
    "            channel_brainLocations[idx].append(row['allen_ontology'])\n",
    "            \n",
    "    # visual discrimination task\n",
    "    goCue_times[idx] = np.load(session + '/trials.goCue_times.npy', allow_pickle = True)\n",
    "    feedback_types[idx] = np.load(session + '/trials.feedbackType.npy', allow_pickle = True)\n",
    "    feedback_times[idx] = np.load(session + '/trials.feedback_times.npy', allow_pickle = True)\n",
    "    response_choices[idx] = np.load(session + '/trials.response_choice.npy', allow_pickle = True)\n",
    "    response_times[idx] = np.load(session + '/trials.response_times.npy', allow_pickle = True)\n",
    "    stimContrast_left[idx] = np.load(session + '/trials.visualStim_contrastLeft.npy', allow_pickle = True)\n",
    "    stimContrast_right[idx] = np.load(session + '/trials.visualStim_contrastRight.npy', allow_pickle = True)\n",
    "    stim_times[idx] = np.load(session + '/trials.visualStim_times.npy', allow_pickle = True)\n",
    "    trial_intervals = np.load(session + '/trials.intervals.npy', allow_pickle = True)\n",
    "    \n",
    "    # behavioral data\n",
    "    eye_area = np.load(session + '/eye.area.npy', allow_pickle = True)\n",
    "    eye_times = np.load(session + '/eye.timestamps.npy', allow_pickle = True)\n",
    "    face_energy = np.load(session + '/face.motionEnergy.npy', allow_pickle = True)\n",
    "    face_times = np.load(session + '/face.timestamps.npy', allow_pickle = True)\n",
    "    \n",
    "    # increment session counter index\n",
    "    idx = idx+1\n",
    "    \n",
    "    \n",
    "print('all data is in dictionaries!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, we can save the dictionaries that we created for variables of interest to their own .npy files, for easy access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving dictionaries to pickle files\n",
    "\n",
    "# wherever you want to save your pickles to\n",
    "os.chdir('your/filepath/here')\n",
    "import pickle\n",
    "\n",
    "dicts_dict = {'spikes_clusters':spikes_clusters, 'spikes_times': spikes_times,'channel_brainLocations':channel_brainLocations,\n", 
    "          'clusters_phy_annotation': clusters_phy_annotation, 'clusters_peakChannel': clusters_peakChannel,\n",
    "          'trial_intervals': trial_intervals, 'goCue_times':goCue_times, 'feedback_types': feedback_types,\n", 
    "          'feedback_times': feedback_times, 'response_choices': response_choices, 'response_times': response_times,\n",
    "          'stimContrast_left': stimContrast_left, 'stimContrast_right': stimContrast_right, 'stim_times': stim_times,\n",
    "          'eye_area': eye_area, 'eye_times': eye_times, 'face_energy': face_energy, 'face_times': face_times}\n",
    "\n",
    "for name,dict_ in dicts_dict.items():\n",
    "\n",
    "    with open(f'{name}.npy', 'wb') as myFile:\n",
    "    pickle.dump(dict_, myFile)\n",
    "    \n",
    "print('all files exported!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
